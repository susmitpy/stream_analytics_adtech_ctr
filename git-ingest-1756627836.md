# 🚀 Project Analysis Report

Directory structure for: stream_analytics

## 🎯 LLM-Optimized Codebase Analysis

This document provides a comprehensive, structured analysis of the codebase optimized for 
Large Language Model (LLM) processing. The content is organized with semantic markup, 
proper syntax highlighting, and hierarchical structure for enhanced AI comprehension.

## 📑 Table of Contents

- [📊 Project Overview](#-project-overview)
- [📈 Statistics](#-statistics)
- [🏗️ Directory Structure](#️-directory-structure)
- [📁 Files by Category](#-files-by-category)
  - [⚙️ Backend/Server](#2699-backend-server)
  - [⚙️ Data/Config](#2699-data-config)
  - [📄 Other](#1f4c4-other)
  - [📖 Documentation](#1f4d6-documentation)
  - [📜 Scripting](#1f4dc-scripting)
- [📋 Complete File Listing](#-complete-file-listing)

## 📊 Project Overview

**Project:** `stream_analytics`  
**Path:** `/Users/susmitvengurlekar/projects/stream_analytics`  
**Generated:** 2025-08-31T08:10:36.544Z  
**Total Files:** 18  
**Total Size:** 26.6 KB  

### 🎯 Quick Summary

This document contains a comprehensive analysis of the **stream_analytics** project, 
including its complete directory structure and the full content of all text files. 
The content is organized in a hierarchical, LLM-friendly format with proper syntax 
highlighting and metadata for optimal AI processing.

## 📈 Statistics

### 📊 File Type Distribution

| Category | Files | Percentage |
| --- | --- | --- |
| Backend/Server | 9 | 50.0% |
| Data/Config | 4 | 22.2% |
| Other | 3 | 16.7% |
| Documentation | 1 | 5.6% |
| Scripting | 1 | 5.6% |

### 💻 Programming Languages

| Language | Files | Primary Category |
| --- | --- | --- |
| go | 6 | Backend/Server |
| text | 3 | Other |
| python | 3 | Backend/Server |
| sql | 3 | Data/Config |
| markdown | 1 | Documentation |
| yaml | 1 | Data/Config |
| bash | 1 | Scripting |

### 📏 Size Analysis

- **Total Project Size:** 26.6 KB
- **Average File Size:** 1.5 KB
- **Text Files:** 18 (100.0%)

## 🏗️ Directory Structure

```
├── flink
│   ├── sql
│   │   ├── tbl_clicks.sql
│   │   ├── tbl_ctr.sql
│   │   └── tbl_impressions.sql
│   ├── ctr.py
│   └── query_loader.py
├── output
├── producer
│   ├── interfaces
│   │   ├── event.go
│   │   └── producer.go
│   ├── models
│   │   ├── click.go
│   │   ├── impression.go
│   │   └── kafka_producer.go
│   ├── go.mod
│   └── main.go
├── docker-compose.yaml
├── Dockerfile.pyflink
├── go.work
├── read_results.py
├── README.md
└── run_demo.sh
```

## 📁 Files by Category

### ⚙️ Backend/Server

**Languages:** python, go  
**File Count:** 9

### ⚙️ Data/Config

**Languages:** yaml, sql  
**File Count:** 4

### 📄 Other

**Languages:** text  
**File Count:** 3

### 📖 Documentation

**Languages:** markdown  
**File Count:** 1

### 📜 Scripting

**Languages:** bash  
**File Count:** 1
## 📋 Complete File Listing

The following section contains the complete content of all text files in the project, 
organized with proper syntax highlighting and metadata for optimal LLM processing.

### 📄 `Dockerfile.pyflink`

**Path:** `Dockerfile.pyflink`  
**Size:** 760 B  
**Language:** text (low confidence)  
**Category:** Other  
```
FROM flink:2.1.0

# Note: default-jdk is installed and JAVA_HOME variable is set for Mac ARM64 architecture compatibility

# install python3 and pip3
RUN apt-get update -y && \
apt-get install -y default-jdk python3 python3-pip python3-dev && rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME=/usr/lib/jvm/default-java

RUN ln -s /usr/bin/python3 /usr/bin/python

# install PyFlink
RUN pip3 install apache-flink==2.1.0

# install additional python dependencies
RUN pip install Jinja2==3.1.6

# download dependencies for connectors
## Kafka (kafka connector does not contain kafka client, hence sql connector is used)
RUN wget -P /opt/flink/lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/4.0.1-2.0/flink-sql-connector-kafka-4.0.1-2.0.jar
```

### 📄 `README.md`

**Path:** `README.md`  
**Size:** 9.6 KB  
**Language:** markdown (medium confidence)  
**Category:** Documentation  
```markdown
# Real-Time Click-Through Rate (CTR) Analysis with Flink & Kafka

This project demonstrates a complete, real-time stream processing pipeline to calculate the Click-Through Rate (CTR) for advertising campaigns. It uses a Go-based data producer to simulate user impressions and clicks, Kafka as the message bus, and a PyFlink job to process the data in real-time.

## 📑 Table of Contents

- [Objective](#objective)
- [Tech Stack](#tech-stack)
- [Usage](#usage)
  - [Prerequisites](#prerequisites)
  - [Running the Demo](#running-the-demo)
  - [Viewing the Results](#viewing-the-results)
- [Components](#components)
  - [1. Go-based Kafka Producer](#1-go-based-kafka-producer)
  - [2. Flink Processing](#2-flink-processing)
    - [2.1. Two Sources](#21-two-sources)
    - [2.2. Interval Join](#22-interval-join)
    - [2.3. Windowed Aggregation](#23-windowed-aggregation)
    - [2.4. Sink](#24-sink)
- [My Learnings](#my-learnings)
  - [1. My First Go Project](#1-my-first-go-project)
  - [2. Crucial Bash Learnings](#2-crucial-bash-learnings)
  - [3. Flink Insights and Gotchas](#3-flink-insights-and-gotchas)
  - [4. AI-Assisted Development](#4-ai-assisted-development)

## Objective

The primary goal of this project is to build a robust data pipeline that can:
1.  Ingest two separate streams of data: `impressions` and `clicks`.
2.  Join these streams in real-time based on a time-bound condition (an impression must be followed by a click within a specific interval).
3.  Aggregate the results over tumbling time windows to calculate the number of impressions, clicks, and the final CTR for each campaign.
4.  Persist the aggregated results to the filesystem for further analysis.

## Tech Stack

*   **Stream Processing:** Apache Flink (PyFlink)
*   **Messaging Platform:** Apache Kafka
*   **Data Producer:** Go
*   **Containerization:** Docker & Docker Compose
*   **Scripting & Tooling:** Bash, Python (Pandas)

## Usage

Follow these steps to get the demo up and running on your local machine.

### Prerequisites

1.  **Go:** You must have the Go programming language installed on your local machine to run the data producer.
2.  **Docker:** You must have Docker and Docker Compose installed to build and run the containerized infrastructure (Kafka, Flink).

### Running the Demo

The entire process is automated with a single script. Open your terminal and run:

```bash
bash run_demo.sh
```

This script will perform the following actions:
1.  **Start Infrastructure:** It launches the Kafka and Flink (JobManager, TaskManager) containers in the background using `docker compose up -d`.
2.  **Create Kafka Topics:** It waits for Kafka to be ready and then creates the necessary topics: `impressions` and `clicks`.
3.  **Submit Flink Job:** The `ctr.py` PyFlink script is submitted to the Flink cluster to start the stream processing.
4.  **Launch Data Producer:** It starts the Go-based data producer, which will begin sending impression and click events to Kafka.
5.  **Monitor:** You can monitor the Flink job's progress via the Flink UI at `http://localhost:8081`.

To stop the demo and clean up all resources, simply press `Ctrl+C` in the terminal where the script is running. The script has a cleanup trap that will automatically shut down and remove the Docker containers.

### Viewing the Results

The Flink job writes its output as CSV files to the `./output/ctr_results` directory. A helper Python script is provided to read these partitioned files. After running the demo for a minute, you can view the results with:

```bash
python read_results.py
```

## Components

The pipeline consists of two main components: the data producer and the Flink processing job.

### 1. Go-based Kafka Producer

The producer is a Go application located in the `producer/` directory. Its sole responsibility is to generate synthetic advertising data and publish it to Kafka.

*   It produces messages to two topics: `impressions` and `clicks`.
*   It generates approximately 5 impressions per second.
*   For each impression, there is a 25% probability that a corresponding `click` event is generated.
*   Clicks are intentionally delayed by a random duration (up to 10 seconds) to simulate real-world user behavior and test the stream processor's ability to handle out-of-order events.

### 2. Flink Processing

The core stream processing logic is defined in `flink/ctr.py`. The pipeline can be broken down into four main stages:

#### 2.1. Two Sources

The pipeline begins by ingesting data from two distinct Kafka topics.
*   **Impressions Source:** Reads from the `impressions` topic.
*   **Clicks Source:** Reads from the `clicks` topic.

#### 2.2. Interval Join

Once the two streams are established, they are joined together using an **Interval Join**. This is a time-aware join that correlates events from both streams based on a time window.

The join condition is twofold:
1.  The `impression ID` must match (`impressions.impr_id == clicks.impr_id`).
2.  The click event must occur within **15 seconds** *after* the impression event.

This ensures that a click is only attributed to an impression if it happens shortly after the impression occurred, filtering out unrelated events.

#### 2.3. Windowed Aggregation

After a successful join, the resulting stream of correlated impression-click pairs is aggregated using a **30-second Tumble Window**.

A tumble (or tumbling) window slices the stream into fixed-size, non-overlapping time windows. For every 30-second window, the job groups the data by `campaign_id` and performs two key aggregations:
*   Counts the number of distinct impressions.
*   Counts the number of distinct clicks.

Finally, it calculates the CTR for each campaign within that window (`clicks / impressions`).

#### 2.4. Sink

The final aggregated results are written to the local filesystem using Flink's **File Sink**.

*   **Connector:** The sink is configured with the `'filesystem'` connector, outputting partitioned CSV files to the `/output/ctr_results` directory.
*   **Partitioning:** The results are partitioned by `campaign_id`, meaning a separate subdirectory is created for each campaign.
*   **Checkpointing for the File Sink:** The File Sink is a transactional sink that relies on Flink's checkpointing mechanism to commit data. In-progress files are written to a temporary directory. Only when a checkpoint is successfully completed does the sink move these files to the final output directory, making them visible. This provides exactly-once processing guarantees, ensuring that data is neither lost nor duplicated in the output, even in the event of a failure. For this reason, checkpointing is explicitly enabled in the job with a 30-second interval: `execution.checkpointing.interval", "30s"`.

## My Learnings

This project was a fantastic learning experience, and I'm walking away with several key insights into the tools and technologies I used.

### 1. My First Go Project
*   **Learning the Language:** I chose Go for the data producer specifically to learn it. Using goroutines to simulate click delays was a perfect, hands-on introduction to its powerful concurrency model.
*   **Clean JSON Handling:** I really liked how Go handles JSON serialization with struct tags. Defining the JSON key directly in the struct (e.g., `CampaignID string \`json:"campaign_id"\``) felt clean and intuitive. As a heavy user of Python's dataclasses, this feature felt both familiar and powerful.

### 2. Crucial Bash Learnings
*   **`trap` for Cleanup:** I learned to use the `trap` command for robust script cleanup, which functions much like Go's `defer`. It ensures my Docker containers always shut down cleanly.
*   **`sh -n` for Dry Runs:** Using `sh -n` to perform syntax checks on my script without executing it was a simple but effective way to catch errors early.

### 3. Flink Insights and Gotchas
*   **Timestamp Precision is Critical:** I learned you must be precise with timestamp data types. I had to explicitly cast numeric epoch values to `TIMESTAMP_LTZ` for Flink's watermarking and time-based joins to function correctly.
*   **Data Columns vs. Logical Aliases:** I discovered a key PyFlink API distinction: use `table_obj.column_name` for clarity when accessing physical data columns. However, for logical constructs created in the plan (like a window defined with `.alias("w")`), you **must** use `E.col("w")` to reference them. This highlights the difference between the data schema and the logical plan.
*   **Append vs. Update Streams:** I gained a clearer understanding of how windowed aggregations are crucial for creating simple, append-only output streams that are compatible with most sinks, as opposed to continuous aggregations which can produce retractions.
*   **Fail Fast, Don't Ignore Errors:** My takeaway is to never configure jobs to ignore parse errors. It's better to let the job fail fast, which immediately signals an upstream data quality issue that needs to be fixed.
*   **Architecture: Flink -> Kafka -> Sink:** I realized a more robust architectural pattern is often to sink Flink results back to a Kafka topic. From there, Kafka Connect can reliably handle delivery to a final destination (like MongoDB), offering more flexibility than native Flink sinks.

### 4. AI-Assisted Development
*   **Git Ingest for Full Context:** The ability to provide my entire codebase as context to the LLM was a massive help. It enabled a much deeper understanding of the project, which made generating documentation and debugging far more efficient.
*   **LLM Choice Matters:** As of August 2025, I found that for PyFlink-specific syntax, Gemini 2.5 Pro was more effective than GPT-5 Thinking, Claude Sonnet 4, or Grok Code Fast 1. This really drove home how an LLM's specific training data impacts its performance on specialized libraries.
```

### 📄 `docker-compose.yaml`

**Path:** `docker-compose.yaml`  
**Size:** 2.1 KB  
**Language:** yaml (medium confidence)  
**Category:** Data/Config  
```yaml
services:

  kafka:
    image: confluentinc/cp-kafka:7.7.1
    container_name: kafka
    ports:
      - "29092:29092"
    environment:
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://localhost:29092,INTERNAL://kafka:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://0.0.0.0:29092,INTERNAL://0.0.0.0:9092,CONTROLLER://kafka:29093'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
    # volumes:
    #   - ./kafka_data:/tmp/kraft-combined-logs


  control-center:
    image: confluentinc/cp-enterprise-control-center:7.7.1
    hostname: control-center
    container_name: control-center
    depends_on:
      - kafka
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'kafka:9092'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021

  jobmanager:
    build:
      context: .
      dockerfile: Dockerfile.pyflink
    expose:
      - "6123"
    ports:
      - "8081:8081"
      - "6123:6123"
    container_name: jobmanager
    volumes:
      - ./flink:/job-src/flink
      - ./output:/job-src/output
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager

  taskmanager:
    build:
      context: .
      dockerfile: Dockerfile.pyflink
    expose:
      - "6121"
      - "6122"
    depends_on:
      - jobmanager
    command: taskmanager
    links:
      - "jobmanager:jobmanager"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
    container_name: taskmanager
    volumes:
      - ./flink:/job-src/flink
      - ./output:/job-src/output
```

### 📄 `ctr.py`

**Path:** `flink/ctr.py`  
**Size:** 2.6 KB  
**Language:** python (medium confidence)  
**Category:** Backend/Server  
```python
from query_loader import render_sql
from pyflink.table import EnvironmentSettings, TableEnvironment, DataTypes
from pyflink.table import expressions as E
from pyflink.table.window import Tumble

def setup_tables(t_env: TableEnvironment):
    """Defines the Kafka source and Filesystem sink tables."""
    t_env.execute_sql(render_sql('tbl_impressions'))
    t_env.execute_sql(render_sql('tbl_clicks'))
    t_env.execute_sql(render_sql('tbl_ctr'))
    

def compute_ctr(t_env: TableEnvironment):
    impressions = t_env.from_path("impressions")
    clicks = t_env.from_path("clicks")

    # MUST use the new 'event_time' column for all time operations
    aliased_impressions = impressions.select(
        impressions.impr_id,
        impressions.campaign_id.alias("impr_campaign_id"),
        impressions.event_time.alias("impr_ts")
    )
    # MUST use the new 'event_time' column here as well
    aliased_clicks = clicks.select(
        clicks.click_id,
        clicks.impr_id.alias("click_impr_id"),
        clicks.event_time.alias("click_ts")
    )

    joined_stream = aliased_impressions.join(aliased_clicks).where(
        (aliased_impressions.impr_id == aliased_clicks.click_impr_id) &
        (aliased_clicks.click_ts.between(
            aliased_impressions.impr_ts,
            aliased_impressions.impr_ts + E.lit(15).seconds
        ))
    )

    windowed_stream = joined_stream.window(
        Tumble.over(E.lit(30).seconds).on(joined_stream.impr_ts).alias("w")
    )

    # Group by window and campaign, then aggregate using aliased columns
    ctr_results = windowed_stream.group_by(
        E.col("w"), joined_stream.impr_campaign_id
    ).select(
        joined_stream.impr_campaign_id.alias("campaign_id"),
        E.col("w").start.alias("window_start"),
        E.col("w").end.alias("window_end"),
        joined_stream.impr_id.count.distinct.alias("impressions"),
        joined_stream.click_id.count.distinct.alias("clicks")
    )

    # Calculate CTR and execute the insert operation
    ctr_results.select(
        ctr_results.campaign_id,
        ctr_results.window_start,
        ctr_results.window_end,
        ctr_results.impressions,
        ctr_results.clicks,
        (ctr_results.clicks.cast(DataTypes.DOUBLE()) / ctr_results.impressions.cast(DataTypes.DOUBLE())).alias("ctr")
    ).execute_insert("ctr_by_campaign").wait()


def main():
    env_settings = EnvironmentSettings.in_streaming_mode()
    t_env = TableEnvironment.create(env_settings)

    t_env.get_config().get_configuration().set_string("execution.checkpointing.interval", "30s")

    setup_tables(t_env=t_env)
    compute_ctr(t_env=t_env)

if __name__ == "__main__":
    main()
```

### 📄 `query_loader.py`

**Path:** `flink/query_loader.py`  
**Size:** 833 B  
**Language:** python (medium confidence)  
**Category:** Backend/Server  
```python
from functools import lru_cache
from pathlib import Path
from jinja2 import Environment, FileSystemLoader, StrictUndefined

BASE_DIR = Path(__file__).resolve().parent
SQL_DIR = BASE_DIR / "sql"

env = Environment(
    loader=FileSystemLoader(str(SQL_DIR)),
    undefined=StrictUndefined,
    autoescape=False,           
    trim_blocks=True,
    lstrip_blocks=True,
)

@lru_cache(maxsize=None)
def load_sql(name: str) -> str:
    """Load raw .sql text by basename (without extension)."""
    path = SQL_DIR / f"{name}.sql"
    if not path.exists():
        raise FileNotFoundError(f"SQL not found: {path}")
    return path.read_text(encoding="utf-8")

def render_sql(name: str, **params) -> str:
    """Render a Jinja-templated .sql with params."""
    template = env.get_template(f"{name}.sql")
    return template.render(**params)
```

### 📄 `tbl_clicks.sql`

**Path:** `flink/sql/tbl_clicks.sql`  
**Size:** 474 B  
**Language:** sql (medium confidence)  
**Category:** Data/Config  
```sql
CREATE TABLE IF NOT EXISTS clicks (
      click_id STRING,
      impr_id STRING,
      user_id STRING,
      ts BIGINT,
      event_time AS TO_TIMESTAMP_LTZ(ts, 3),
      WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
    ) WITH (
      'connector' = 'kafka',
      'topic' = 'clicks',
      'properties.bootstrap.servers' = 'kafka:9092',
      'properties.group.id' = 'pyflink-ctr',
      'scan.startup.mode' = 'earliest-offset',
      'format' = 'json'
    )
```

### 📄 `tbl_ctr.sql`

**Path:** `flink/sql/tbl_ctr.sql`  
**Size:** 677 B  
**Language:** sql (medium confidence)  
**Category:** Data/Config  
```sql
CREATE TABLE IF NOT EXISTS ctr_by_campaign (
      campaign_id STRING,
      window_start TIMESTAMP_LTZ(3),
      window_end   TIMESTAMP_LTZ(3),
      impressions BIGINT,
      clicks      BIGINT,
      ctr         DOUBLE
    ) PARTITIONED BY (campaign_id) WITH (
      'connector' = 'filesystem',
      'path' = '/job-src/output/ctr_results',
      'format' = 'csv',
      'csv.field-delimiter' = ',',
      'sink.rolling-policy.file-size' = '128MB',
      'sink.rolling-policy.rollover-interval' = '15 min',
      'sink.partition-commit.trigger' = 'process-time',
      'sink.partition-commit.delay' = '1 min',
      'sink.partition-commit.policy.kind' = 'success-file'
    )
```

### 📄 `tbl_impressions.sql`

**Path:** `flink/sql/tbl_impressions.sql`  
**Size:** 487 B  
**Language:** sql (medium confidence)  
**Category:** Data/Config  
```sql
CREATE TABLE IF NOT EXISTS impressions (
      impr_id STRING,
      user_id STRING,
      campaign_id STRING,
      ts BIGINT,
      event_time AS TO_TIMESTAMP_LTZ(ts, 3),
      WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
    ) WITH (
      'connector' = 'kafka',
      'topic' = 'impressions',
      'properties.bootstrap.servers' = 'kafka:9092',
      'properties.group.id' = 'pyflink-ctr',
      'scan.startup.mode' = 'earliest-offset',
      'format' = 'json'
    )
```

### 📄 `go.work`

**Path:** `go.work`  
**Size:** 26 B  
**Language:** text (low confidence)  
**Category:** Other  
```
go 1.24.2

use ./producer
```

### 📄 `go.mod`

**Path:** `producer/go.mod`  
**Size:** 224 B  
**Language:** text (low confidence)  
**Category:** Other  
```
module github.com/susmitpy/stream_analytics_ctr/producer

go 1.24

require github.com/segmentio/kafka-go v0.4.48

require (
	github.com/klauspost/compress v1.15.9 // indirect
	github.com/pierrec/lz4/v4 v4.1.15 // indirect
)
```

### 📄 `event.go`

**Path:** `producer/interfaces/event.go`  
**Size:** 99 B  
**Language:** go (medium confidence)  
**Category:** Backend/Server  
```go
package interfaces

type Event interface {
	Key() []byte
	Topic() string
	Value() ([]byte, error)
}
```

### 📄 `producer.go`

**Path:** `producer/interfaces/producer.go`  
**Size:** 125 B  
**Language:** go (medium confidence)  
**Category:** Backend/Server  
```go
package interfaces

import "context"

type Producer interface {
	Write(ctx context.Context, e Event) error
	Close() []error
}
```

### 📄 `main.go`

**Path:** `producer/main.go`  
**Size:** 2.5 KB  
**Language:** go (medium confidence)  
**Category:** Backend/Server  
```go
package main

import (
	"context"
	"fmt"
	"math/rand"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	"github.com/susmitpy/stream_analytics_ctr/producer/models"
)

func randomId(prefix string, n int) string {
	const letters = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
	const numLetters = len(letters)
	b := make([]byte, n)
	for i := range b {
		b[i] = letters[rand.Intn(numLetters)]
	}
	return prefix + "-" + string(b)
}

var campaigns = []string{
	"campaign-1",
	"campaign-2",
	"campaign-3",
}

func main() {
	fmt.Println("Starting synthetic producer")

	// graceful shutdown
	ctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt, syscall.SIGTERM)
	defer cancel()

	brokers := []string{"localhost:29092"}
	kp := models.NewKafkaProducer(brokers, "impressions", "clicks")
	// ensure writers are closed on exit
	defer kp.Close()

	// generator configuration
	impressionsPerSecond := 5
	clickProbability := 0.25 // probability an impression will generate a click
	maxClickDelaySeconds := 10 // max delay before a click is generated after an impression

	var wg sync.WaitGroup

	ticker := time.NewTicker(time.Second / time.Duration(impressionsPerSecond))
	defer ticker.Stop()

	fmt.Printf("Producing impressions at ~%d/sec, clickProb=%.2f\n", impressionsPerSecond, clickProbability)

	LOOP:
	for {
		select {
		case <-ctx.Done():
			break LOOP
		case <-ticker.C:
			impr := models.Impression{
				ImprId: randomId("impr", 8),
				UserId: randomId("user", 6),
				CampaignID: campaigns[rand.Intn(len(campaigns))],
				Ts:     time.Now().UnixMilli(),
			}
			if err := kp.Write(ctx, impr); err != nil {
				fmt.Printf("failed to write impression: %v\n", err)
			}

			// randomly schedule a click for this impression
			if rand.Float64() < clickProbability {
				wg.Add(1)
				go func(im models.Impression) {
					defer wg.Done()
					delay := time.Duration(rand.Intn(maxClickDelaySeconds+1)) * time.Second
					select {
					case <-time.After(delay):
						click := models.Click{
							ClickId: randomId("click", 8),
							ImpreId: im.ImprId,
							UserId:  im.UserId,
							TS:      time.Now().UnixMilli(),
						}
						if err := kp.Write(ctx, click); err != nil {
							fmt.Printf("failed to write click: %v\n", err)
						}
					case <-ctx.Done():
						// on shutdown, don't attempt to write late clicks
						return
					}
				}(impr)
			}
		}
	}

	// wait for any in-flight click writers to finish
	fmt.Println("shutting down generator, waiting for in-flight clicks...")
	wg.Wait()
	fmt.Println("shutdown complete")
}
```

### 📄 `click.go`

**Path:** `producer/models/click.go`  
**Size:** 381 B  
**Language:** go (medium confidence)  
**Category:** Backend/Server  
```go
package models

import (
	"encoding/json"
)

type Click struct {
	ClickId string `json:"click_id"`
	ImpreId string `json:"impr_id"`
	UserId string `json:"user_id"`
	TS int64 `json:"ts"`
}

// Interface: Event
func (c Click) Key() []byte { return []byte(c.ClickId) }
func (c Click) Topic() string { return "clicks" }
func (c Click) Value() ([]byte, error) { return json.Marshal(c) }
```

### 📄 `impression.go`

**Path:** `producer/models/impression.go`  
**Size:** 414 B  
**Language:** go (medium confidence)  
**Category:** Backend/Server  
```go
package models

import (
	"encoding/json"
)

type Impression struct {
	ImprId string `json:"impr_id"`
	UserId string `json:"user_id"`
	CampaignID string    `json:"campaign_id"`
	Ts int64 `json:"ts"`
}

// Interface: Event
func (i Impression) Key() []byte { return []byte(i.ImprId) }
func (i Impression) Topic() string { return "impressions" }
func (i Impression) Value() ([]byte, error) { return json.Marshal(i) }
```

### 📄 `kafka_producer.go`

**Path:** `producer/models/kafka_producer.go`  
**Size:** 1 KB  
**Language:** go (medium confidence)  
**Category:** Backend/Server  
```go
package models

import (
	"context"

	"github.com/segmentio/kafka-go"
	"github.com/susmitpy/stream_analytics_ctr/producer/interfaces"
)

type KafkaProducer struct {
	writers map[string]*kafka.Writer
}

// Interface: Producer
func (kp *KafkaProducer) Write(ctx context.Context, e interfaces.Event) error {
	val, err := e.Value()
	if err != nil {
		return err
	}

	msg := kafka.Message{
		Key:  e.Key(),
		Value: val,
	}

	return kp.writers[e.Topic()].WriteMessages(ctx, msg)
}

func (kp *KafkaProducer) Close() []error {
	var errs []error
	for _, writer := range kp.writers {
		if err := writer.Close(); err != nil {
			errs = append(errs, err)
		}
	}
	return errs
}

func NewKafkaProducer(brokers []string, topics ...string) *KafkaProducer {
	writers := make(map[string]*kafka.Writer)
	for _, topic := range topics {
		writers[topic] = &kafka.Writer{
			Addr:     kafka.TCP(brokers...),
			Topic:    topic,
			RequiredAcks: kafka.RequireAll,
			Balancer: &kafka.Murmur2Balancer{},
		}
	}
	return &KafkaProducer{
		writers: writers,
	}

}
```

### 📄 `read_results.py`

**Path:** `read_results.py`  
**Size:** 2.6 KB  
**Language:** python (medium confidence)  
**Category:** Backend/Server  
```python
import pandas as pd
import glob
import os

def read_flink_output_no_header(base_path: str) -> pd.DataFrame:
    """
    Reads partitioned, header-less CSV files produced by a Flink FileSink,
    handling Flink's default file naming (no .csv extension).

    Args:
        base_path: The root directory of the Flink sink output
                   (e.g., 'output/ctr_results').

    Returns:
        A single pandas DataFrame containing the data from all partitions.
    """
    # Define the column names that match your Flink sink table DDL
    column_names = [
        'window_start',
        'window_end',
        'impressions',
        'clicks',
        'ctr'
    ]

    # --- THE KEY CHANGE IS HERE ---
    # The glob pattern now looks for files starting with 'part-' and does
    # not require a .csv extension. This matches Flink's default naming.
    file_pattern = os.path.join(base_path, '**', 'part-*')
    
    # We use glob to find all part-files recursively.
    part_files = glob.glob(file_pattern, recursive=True)

    if not part_files:
        print(f"No committed part-files found in '{base_path}'.")
        print("Make sure files are committed (not '.inprogress') and the path is correct.")
        return pd.DataFrame()

    all_dataframes = []

    print(f"Found {len(part_files)} part-files to read...")

    for file_path in part_files:
        # Ignore any potential directories that might match the pattern
        if not os.path.isfile(file_path):
            continue

        # Read the individual file, applying the manual header
        df_part = pd.read_csv(
            file_path,
            header=None,
            names=column_names
        )

        # Extract Partition Information from the Path (logic is unchanged)
        parent_dir = os.path.basename(os.path.dirname(file_path))
        if '=' in parent_dir:
            partition_key, partition_value = parent_dir.split('=', 1)
            df_part[partition_key] = partition_value

        all_dataframes.append(df_part)

    # If no valid files were processed, return an empty DataFrame
    if not all_dataframes:
        print("Warning: Pattern matched some paths, but none were valid files.")
        return pd.DataFrame()

    # Concatenate all the individual DataFrames into a single one
    final_df = pd.concat(all_dataframes, ignore_index=True)

    return final_df

if __name__ == '__main__':
    results_path = 'output/ctr_results'

    ctr_data = read_flink_output_no_header(results_path)

    if not ctr_data.empty:
        print("\nSuccessfully loaded the data into a single DataFrame:")
        print(ctr_data.head())
```

### 📄 `run_demo.sh`

**Path:** `run_demo.sh`  
**Size:** 1.9 KB  
**Language:** bash (medium confidence)  
**Category:** Scripting  
```bash
#!/usr/bin/env bash
set -euo pipefail

# This function will be called when the script exits, ensuring a clean shutdown.
function cleanup() {
  echo "" # Newline for cleaner output
  echo "Shutting down Docker containers..."
  # Use --volumes to also remove the anonymous volumes created by Kafka
  docker compose down --volumes
}

# 'trap' sets up a command to run when the script receives a signal.
# We trap EXIT, INT (Ctrl+C), and TERM signals to run our cleanup function.
trap cleanup EXIT INT TERM

# Set up paths
ROOT_DIR="$(pwd)"
FLINK_DIR="$ROOT_DIR/flink"
PYFILES_ZIP="$ROOT_DIR/flink/flink_job_deps.zip"
JOB_SCRIPT="/job-src/flink/ctr.py"

cd "$ROOT_DIR"

rm -rf output
mkdir -p output

# 1. Start the docker-compose cluster in the background
echo "Starting Flink + Kafka cluster with 'docker compose up -d'..."
docker compose up -d

# Give Kafka a moment to be ready before creating topics
echo "Waiting for Kafka to be ready..."
sleep 5

# 2. Create Kafka topics
echo "Creating Kafka topics: 'impressions' and 'clicks'..."
docker exec -it kafka bash -lc '
kafka-topics --bootstrap-server kafka:29092 --create --topic impressions --partitions 3 --replication-factor 1 --if-not-exists
kafka-topics --bootstrap-server kafka:29092 --create --topic clicks --partitions 3 --replication-factor 1 --if-not-exists
'

echo "Submitting Flink CTR job to the cluster..."
docker exec jobmanager /opt/flink/bin/flink run -d -py "$JOB_SCRIPT" --pyFiles /job-src/flink/flink_job_deps.zip &

echo ""
echo "✅ Flink job submitted successfully!"
echo "➡️ Flink UI: http://localhost:8081"
echo "➡️ Confluent Control Center: http://localhost:9021"
echo ""

# 3. Start the Go data producer in the foreground
echo "🚀 Starting data producer. Press [Ctrl+C] to stop."
echo "----------------------------------------------------"
go run ./producer/main.go

# The 'trap' will automatically call the 'cleanup' function when you press Ctrl+C
```

---

### 📊 Processing Summary

- **Files Processed:** 18
- **Files Skipped:** 0
- **Total Files:** 18
- **Concurrency Limit:** 3
